{
	"name": "Project1_ForestFire_PL",
	"properties": {
		"activities": [
			{
				"name": "pysparkTransform",
				"type": "DatabricksNotebook",
				"dependsOn": [
					{
						"activity": "VerifySourceFile",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebookPath": "/Users/keerthanaakkula@gmail.com/LogicTransformation.py",
					"baseParameters": {
						"csvpath": {
							"value": "@pipeline().parameters.inputfilepath",
							"type": "Expression"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "AzureDatabricks1",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "Getoutputfile",
				"type": "SetVariable",
				"dependsOn": [
					{
						"activity": "pysparkTransform",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"variableName": "setoutputvariable",
					"value": {
						"value": "@activity('pysparkTransform').output.runOutput",
						"type": "Expression"
					}
				}
			},
			{
				"name": "CopyToSynapse",
				"description": "This activity will copy the transformed data from adls to synapse",
				"type": "Copy",
				"dependsOn": [
					{
						"activity": "Getoutputfile",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "DelimitedTextSource",
						"storeSettings": {
							"type": "AzureBlobFSReadSettings",
							"recursive": true,
							"wildcardFolderPath": {
								"value": "@split(split(variables('setoutputvariable'), '@')[1], '/')[1]",
								"type": "Expression"
							},
							"wildcardFileName": {
								"value": "@split(variables('setoutputvariable'), '/')[sub(length(split(variables('setoutputvariable'), '/')), 1)]\n",
								"type": "Expression"
							},
							"enablePartitionDiscovery": false
						},
						"formatSettings": {
							"type": "DelimitedTextReadSettings"
						}
					},
					"sink": {
						"type": "SqlDWSink",
						"writeBehavior": "Insert",
						"sqlWriterUseTableLock": false
					},
					"enableStaging": false,
					"translator": {
						"type": "TabularTranslator",
						"typeConversion": true,
						"typeConversionSettings": {
							"allowDataTruncation": true,
							"treatBooleanAsNumber": false
						}
					}
				},
				"inputs": [
					{
						"referenceName": "SourceDataset",
						"type": "DatasetReference"
					}
				],
				"outputs": [
					{
						"referenceName": "sinkDataset",
						"type": "DatasetReference"
					}
				]
			},
			{
				"name": "VerifySourceFile",
				"description": "Verify if the source file is present or not in Azure Data Lake Storage ",
				"type": "IfCondition",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"expression": {
						"value": "@equals(pipeline().parameters.inputfilepath, pipeline().parameters.inputfilepath)",
						"type": "Expression"
					},
					"ifFalseActivities": [
						{
							"name": "Fail1",
							"description": "This fail activity will fail the pipleline if the input file is not present",
							"type": "Fail",
							"dependsOn": [],
							"userProperties": [],
							"typeProperties": {
								"message": "This pipleline as no inputfile",
								"errorCode": "500"
							}
						}
					]
				}
			}
		],
		"parameters": {
			"inputfilepath": {
				"type": "string"
			}
		},
		"variables": {
			"setoutputvariable": {
				"type": "String"
			}
		},
		"annotations": []
	}
}